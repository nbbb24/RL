- algorithm.adv_estimator=grpo
- trainer.val_before_train=False
- data.train_files=data/processed/ECG_Knowledge_Basic_Q_A_grpo.parquet
- data.val_files=data/processed/ECG_Knowledge_Basic_Q_A_val.parquet
- data.train_batch_size=16
- data.max_prompt_length=512
- data.max_response_length=1024
- data.filter_overlong_prompts=True
- data.truncation=error
- data.shuffle=False
- actor_rollout_ref.model.path=meta-llama/Llama-3.2-3B-Instruct
- actor_rollout_ref.model.lora_rank=8
- actor_rollout_ref.model.lora_alpha=16
- actor_rollout_ref.actor.optim.lr=3e-6
- actor_rollout_ref.model.use_remove_padding=True
- actor_rollout_ref.actor.ppo_mini_batch_size=16
- actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8
- actor_rollout_ref.actor.use_kl_loss=True
- actor_rollout_ref.actor.kl_loss_coef=0.001
- actor_rollout_ref.actor.kl_loss_type=low_var_kl
- actor_rollout_ref.actor.entropy_coeff=0
- actor_rollout_ref.model.enable_gradient_checkpointing=True
- actor_rollout_ref.actor.fsdp_config.param_offload=False
- actor_rollout_ref.actor.fsdp_config.optimizer_offload=False
- actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=40
- actor_rollout_ref.rollout.tensor_model_parallel_size=1
- actor_rollout_ref.rollout.name=vllm
- actor_rollout_ref.rollout.gpu_memory_utilization=0.6
- actor_rollout_ref.rollout.n=2
- actor_rollout_ref.rollout.load_format=safetensors
- actor_rollout_ref.rollout.layered_summon=True
- actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=40
- actor_rollout_ref.ref.fsdp_config.param_offload=True
- algorithm.use_kl_in_reward=False
- trainer.critic_warmup=0
- trainer.logger=["console","wandb"]
- trainer.project_name=ecg_rl
- trainer.experiment_name=llama3.2_3b_grpo_lora
- trainer.n_gpus_per_node=1
- trainer.nnodes=1
- trainer.save_freq=20
- trainer.test_freq=5
- trainer.default_local_dir=models/grpo/meta-llama/Llama-3.2-3B-Instruct
- trainer.total_epochs=1
